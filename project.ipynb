{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785fc004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from BTrees.OOBTree import OOBTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99126733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista dei postings aka i docID\n",
    "class PostingsList:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._postings_list = []\n",
    "\n",
    "    # crea una PostingsList da una lista di docID e la ordina (forse non necssario)\n",
    "    @classmethod\n",
    "    def from_postings_list(cls, postings_list: list[int]) -> 'PostingsList':\n",
    "        plist = cls()\n",
    "        postings_list.sort()\n",
    "        plist._postings_list = postings_list\n",
    "        return plist\n",
    "\n",
    "    # Crea una PostingsList da un singolo docID\n",
    "    @classmethod\n",
    "    def from_doc_id(cls, doc_id: int) -> 'PostingsList':\n",
    "        plist = cls()\n",
    "        plist._postings_list = [doc_id]\n",
    "        return plist\n",
    "\n",
    "    # Concatena due PostingsList. Le liste sono ordinate e i duplicati rimossi. other contiene una PostingsList creata successivamente a self (i docID saranno piu grandi o uguali)\n",
    "    def merge(self, other: \"PostingsList\") -> 'PostingsList':\n",
    "        if self._postings_list == []:\n",
    "            self._postings_list = other._postings_list\n",
    "        i = 0  # Start index for the other PostingList.\n",
    "        last = self._postings_list[-1]  # The last Posting in the current list.\n",
    "        # Loop through the other PostingList and skip duplicates.\n",
    "        while (i < len(other._postings_list) and last == other._postings_list[i]):\n",
    "            i += 1  # Increment the index if a duplicate is found.\n",
    "        # Append the non-duplicate postings from the other list.\n",
    "        self._postings_list += other._postings_list[i:]\n",
    "        return self\n",
    "\n",
    "    # Ottiene i titoli di documenti dai docID nella PostingsList\n",
    "    def get_from_corpus(self, corpus) -> list[str]:\n",
    "        return list(map(lambda x: corpus[x], self._postings_list))\n",
    "\n",
    "    # Effettua l'intersezione di due PostgingsList con il metodo del doppio indice\n",
    "    def intersection(self, other: \"PostingsList\") -> 'PostingsList':\n",
    "        plist = []\n",
    "        i = 0  # indice riferito a self\n",
    "        j = 0  # indice riferito a other\n",
    "        # finché non si eccede la dimensione di ciascuna lista:\n",
    "        while (i < len(self._postings_list)) and (j < len(other._postings_list)):\n",
    "            # se c'e' un match aggiungi l'elemento e incrmeneta entrambi\n",
    "            if self._postings_list[i] == other._postings_list[j]:\n",
    "                plist.append(self._postings_list[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            # altrimenti aumenta il piu piccolo dei due\n",
    "            elif self._postings_list[i] <= other._postings_list[j]:\n",
    "                i += 1\n",
    "            # altrimenti aumenta l'altro\n",
    "            else:\n",
    "                j += 1\n",
    "        return PostingsList.from_postings_list(plist)\n",
    "\n",
    "    # Effettua l'unione di due PostingsList con il metodo del doppio indice\n",
    "    def union(self, other: \"PostingsList\") -> 'PostingsList':\n",
    "        plist = []\n",
    "        i = 0  # indice riferito a self\n",
    "        j = 0  # indice riferito a other\n",
    "        # fintanto che gli indici sono piu' piccoli di entrambe le liste\n",
    "        while (i < len(self._postings_list)) and (j < len(other._postings_list)):\n",
    "            # aggiungi il docID e aumenta entrambi\n",
    "            if self._postings_list[i] == other._postings_list[j]:\n",
    "                plist.append(self._postings_list[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            # altrimenti aggiungi il docID e aumenta il piu' piccolo\n",
    "            elif self._postings_list[i] < other._postings_list[j]:\n",
    "                plist.append(self._postings_list[i])\n",
    "                i += 1\n",
    "            #  aggiungi l'altro e incrementalo\n",
    "            else:\n",
    "                plist.append(other._postings_list[j])\n",
    "                j += 1\n",
    "        # aggiungi la porzione restante di lista\n",
    "        if i < len(self._postings_list):  # nel caso in cui self era piu' lunga\n",
    "            plist += self._postings_list[i:]\n",
    "        elif j < len(other._postings_list):  # nel caso in cui other era piu' lunga\n",
    "            plist += other._postings_list[j:]\n",
    "        return PostingsList.from_postings_list(plist)\n",
    "\n",
    "    # Effettua la negazione del tipo AND NOT con il metodo dei due indici\n",
    "    def negation(self, other: 'PostingsList') -> 'PostingsList':\n",
    "        plist = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while (i < len(self._postings_list)) and (j < len(other._postings_list)):\n",
    "            # se self contiene il docID, scartalo e incrementa entrambi\n",
    "            if self._postings_list[i] == other._postings_list[j]:\n",
    "                i += 1\n",
    "                j += 1\n",
    "            # aggiungi il docID da self e incrementa se e' piu' piccolo\n",
    "            elif self._postings_list[i] < other._postings_list[j]:\n",
    "                plist.append(self._postings_list[i])\n",
    "                i += 1\n",
    "            # incrementa other\n",
    "            else:\n",
    "                j += 1\n",
    "        # aggiungi i documenti mancanti da self\n",
    "        if i < len(self._postings_list):  # se e' piu' lungo di other\n",
    "            plist += self._postings_list[i:]\n",
    "        return PostingsList.from_postings_list(plist)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \", \".join(map(str, self._postings_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b34d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \"\"\"Remove punctuation and convert text to lowercase\"\"\"\n",
    "    return re.sub(r'[^\\w\\s^-]', '', text).lower()\n",
    "\n",
    "\n",
    "def tokenize(content) -> list:\n",
    "    \"\"\"Split normalized text into tokens\"\"\"\n",
    "    return normalize(content).split()\n",
    "\n",
    "\n",
    "class InvertedIndex:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.btree = OOBTree()  # usa un Btree per rendere piu' veloci aggiornamenti dell'indice\n",
    "\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, max_size=0) -> 'InvertedIndex':\n",
    "        terms = {}  # dizionario temporaneo per tenere l'indice iniziale\n",
    "        # per ogni documento\n",
    "        for doc_id, content in enumerate(tqdm(corpus, total=max_size or None)):\n",
    "            # crea un set dei termini che contiene\n",
    "            tokens = set(tokenize(content.description))\n",
    "            for token in tokens:  # per ogni termine\n",
    "                plist = PostingsList.from_doc_id(doc_id)\n",
    "                if token in terms:  # se contenuto\n",
    "                    terms[token].merge(plist)  # fai merge delle PostingsList\n",
    "                else:  # altrimenti aggiungi\n",
    "                    terms[token] = plist\n",
    "        idx = cls()\n",
    "        idx.btree.update(terms)\n",
    "        return idx\n",
    "\n",
    "    # crea il biword index per le phrase queries\n",
    "    @classmethod\n",
    "    def from_corpus_biword(cls, corpus, max_size=0) -> 'InvertedIndex':\n",
    "        terms = {}\n",
    "        # per ogni documento\n",
    "        for doc_id, content in enumerate(tqdm(corpus, total=max_size or None)):\n",
    "            tokens = tokenize(content.description)\n",
    "            # per ogni parola\n",
    "            for i in range(len(tokens) - 1):\n",
    "                biword = tokens[i]+tokens[i+1]\n",
    "                plist = PostingsList.from_doc_id(doc_id)\n",
    "                if biword in terms:\n",
    "                    terms[biword].merge(plist)\n",
    "                else:\n",
    "                    terms[biword] = plist\n",
    "        idx = cls()\n",
    "        idx.btree.update(terms)\n",
    "        return idx\n",
    "\n",
    "    def merge(self, other: 'InvertedIndex') -> 'InvertedIndex':\n",
    "        for term, postings in other.btree.items():\n",
    "            if term in self.btree:\n",
    "                self.btree[term].merge(postings)\n",
    "            else:\n",
    "                self.btree[term] = postings\n",
    "        return self\n",
    "\n",
    "    def __getitem__(self, key: str) -> PostingsList:\n",
    "        return self.btree[key]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.btree)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.btree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDescription:\n",
    "    def __init__(self, title: str, description: str) -> None:\n",
    "        self.title = title\n",
    "        self.description = description\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.title\n",
    "\n",
    "# leggi il file descrizione e metadata e crea un corpus (collection di documenti)\n",
    "\n",
    "\n",
    "def read_movie_description(movie_metadata, description_file) -> list[MovieDescription]:\n",
    "    names = {}\n",
    "    corpus = []\n",
    "    with open(movie_metadata, 'r') as file:  # leggi i metadati\n",
    "        movie_names = csv.reader(file, delimiter='\\t')\n",
    "        for description in movie_names:  # aggiungi a names la coppia id_film: titolo\n",
    "            names[description[0]] = description[2]\n",
    "    with open(description_file, 'r') as file:  # leggi le descrizioni\n",
    "        descriptions = csv.reader(file, delimiter='\\t')\n",
    "        for description in descriptions:\n",
    "            try:\n",
    "                # aggiungi al corpus il titolo e la descrizione di ciascun film\n",
    "                corpus.append(MovieDescription(\n",
    "                    # il docID e' la posizione del documento nel corpus\n",
    "                    names[description[0]], description[1]))\n",
    "            except KeyError:\n",
    "                pass\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9731a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrSystem:\n",
    "    def __init__(self, corpus: list[MovieDescription], index: InvertedIndex, biword: InvertedIndex, max_size_aux=10000) -> None:\n",
    "        self._corpus = corpus\n",
    "        self._index = index  # inverted index\n",
    "        self._invalid_vec = []  # invalidation bit vector\n",
    "        self._temp_idx = None  # indice ausiliario\n",
    "        self.max_size_aux = max_size_aux  # massimo docID assegnato\n",
    "        self._biword = biword  # inverted index con biword per phrase queries\n",
    "        self._temp_biword = None\n",
    "\n",
    "    # Crea l'indice e il biword\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus: list[MovieDescription]) -> 'IrSystem':\n",
    "        index = InvertedIndex.from_corpus(corpus)\n",
    "        biword = InvertedIndex.from_corpus_biword(corpus)\n",
    "        ir = cls(corpus, index, biword)\n",
    "        ir._invalid_vec = [0] * len(corpus)\n",
    "        return ir\n",
    "\n",
    "    # Segna documenti cancellati\n",
    "    def delete_docs(self, documents: list[int]) -> 'IrSystem':\n",
    "        for doc in documents:\n",
    "            self._invalid_vec[doc] = 1\n",
    "        return self\n",
    "\n",
    "    # Aggiungi documenti nuovi all'indice ausiliario\n",
    "    def add_docs(self, corpus: list[MovieDescription]) -> 'IrSystem':\n",
    "        # i nuovi documenti usano docID piu' grandi\n",
    "        aux = InvertedIndex.from_corpus(corpus, len(self._invalid_vec))\n",
    "        if self._temp_idx is None:  # se non e' presente nell'indice ausiliario\n",
    "            self._temp_idx = aux  # aggiungilo\n",
    "        else:  # altrimenti\n",
    "            self._temp_idx.merge(aux)\n",
    "        if len(self._temp_idx) > self.max_size_aux:  # se l'indice ausiliario e' troppo grande\n",
    "            self.merge_idx()  # fai merge\n",
    "\n",
    "        aux_biword = InvertedIndex.from_corpus_biword(corpus, len(self.invali))\n",
    "        if self._temp_biword is None:\n",
    "            self._temp_biword = aux_biword\n",
    "        else:\n",
    "            self._temp_biword.merge(aux_biword)\n",
    "        # aggiorna la dimensione massima attuale\n",
    "        self.max_size_aux += len(corpus)\n",
    "        # aggiorna l'invalidation bit vector\n",
    "        self._invalid_vec += [0] * len(corpus)\n",
    "        return self\n",
    "\n",
    "    # Merge dell'indice ausilario con l'InvertedIndex\n",
    "    def merge_idx(self) -> 'IrSystem':\n",
    "        self._index.merge(self._temp_idx)\n",
    "        self._biword.merge(self._temp_biword)\n",
    "        self._temp_idx = None\n",
    "        self._temp_biword = None\n",
    "        return self\n",
    "\n",
    "    # Effettua una query booleana combinando i termini con AND, OR e NOT\n",
    "    def query(self, query: str) -> list[str]:\n",
    "        tokens = query.split()\n",
    "        # riscrive la query con gli operatori postfix\n",
    "        postfix = infix_to_postfix(tokens)\n",
    "        stack = []  # PostingsList ancora da processare\n",
    "        for token in postfix:\n",
    "            if token in ('AND', 'OR', 'NOT'):\n",
    "                right = stack.pop()\n",
    "                left = stack.pop()\n",
    "                if token == 'AND':  # caso AND, conviene ottimizzare la query facendo l'intersezione delle liste piu' corte in primis\n",
    "                    if not isinstance(left, list):\n",
    "                        left = [left]\n",
    "                    if not isinstance(right, list):\n",
    "                        right = [right]\n",
    "                    # aggiungi allo stack una lista [left, right]\n",
    "                    stack.append(left + right)\n",
    "                elif token in ('OR', 'NOT'):\n",
    "                    if isinstance(left, list):  # se left e' una lista (catena di AND)\n",
    "                        # effettua la sequenza di AND\n",
    "                        left = self._optimize_and_query(left)\n",
    "                    if isinstance(right, list):  # se right e' una lista (catena di AND)\n",
    "                        # effettua la sequenza di AND\n",
    "                        right = self._optimize_and_query(right)\n",
    "                    if token == 'OR':  # effettua l'OR\n",
    "                        stack.append(left.union(right))\n",
    "                    else:  # effettua il NOT (AND NOT)\n",
    "                        stack.append(left.negation(right))\n",
    "            else:  # aggiungi una PostingsList da processare allo stack\n",
    "                base = self._index.btree.get(\n",
    "                    token, PostingsList.from_postings_list([]))\n",
    "                aux = self._temp_idx.btree.get(token, PostingsList.from_postings_list(\n",
    "                    [])) if self._temp_idx else PostingsList.from_postings_list([])\n",
    "                stack.append(base.merge(aux))\n",
    "        result = stack.pop()  # estrai l'ultimo elemento (risultato finale)\n",
    "        if isinstance(result, list):  # se e' tuttora una lista (= catena di AND), fai l'intersezione\n",
    "            result = self._optimize_and_query(result)\n",
    "        # elimina i documenti cancellati\n",
    "        return self._remove_deleted(result).get_from_corpus(self._corpus)\n",
    "\n",
    "    def _remove_deleted(self, result: PostingsList) -> PostingsList:\n",
    "        for doc_id, deleted in enumerate(self._invalid_vec):\n",
    "            if deleted and doc_id in result._postings_list:\n",
    "                result._postings_list.remove(doc_id)\n",
    "        return result\n",
    "\n",
    "    # Effettua operazioni di AND consecutive facendo l'intersezione di PostingsList piu' corte prima\n",
    "    def _optimize_and_query(self, terms: list[PostingsList]) -> PostingsList:\n",
    "        # ordina le PostingsList per lunghezza crescente\n",
    "        plist = sorted(terms, key=lambda x: len(x._postings_list))\n",
    "        result = reduce(lambda x, y: x.intersection(y), plist)\n",
    "        return result\n",
    "\n",
    "    # Ricerca una sequenza specifica di parola nel corpus con biword\n",
    "    def phrase_query(self, query: str) -> list[str]:\n",
    "        biword_query = []\n",
    "        words = query.split()\n",
    "        for i in range(len(words)-1):\n",
    "            # concatena le parole della query in coppie\n",
    "            biword_query.append(words[i]+words[i+1])\n",
    "        postings = []\n",
    "        # cerca le biword nel biword index\n",
    "        for biword in biword_query:\n",
    "            base = self._biword.btree.get(\n",
    "                biword, PostingsList.from_postings_list([]))\n",
    "            aux = self._temp_biword.btree.get(biword, PostingsList.from_postings_list(\n",
    "                [])) if self._temp_biword else PostingsList.from_postings_list([])\n",
    "            postings.append(base.merge(aux))\n",
    "        # effettua l'intersezione delle PostingsList trovate\n",
    "        plist = reduce(lambda x, y: x.intersection(y), postings)\n",
    "        # rimuovi cancellati e ritorna i risultati\n",
    "        return self._remove_deleted(plist).get_from_corpus(self._corpus)\n",
    "\n",
    "# Rende una espressione da infix a postfix: a AND b OR c -> a b AND c OR\n",
    "\n",
    "\n",
    "def infix_to_postfix(tokens: list[str]) -> list[str]:\n",
    "    output = []  # risultato finale\n",
    "    stack = []  # ancora da processare\n",
    "    for token in tokens:\n",
    "        if token in ('AND', 'OR', 'NOT'):  # se e' un operatore\n",
    "            # finche' ci sono parole da processare e non e' una parentesi\n",
    "            while (stack and stack[-1] != '('):\n",
    "                # aggiungi al risultato finale le parole una dopo l'altra\n",
    "                output.append(stack.pop())\n",
    "            stack.append(token)  # aggiungi l'operatore allo stack\n",
    "        elif token == '(':  # aggiungi la parentesi allo stack\n",
    "            stack.append(token)\n",
    "        elif token == ')':\n",
    "            # fino a che non incontro la parentesi aperta o si svuota lo stack\n",
    "            while stack and stack[-1] != '(':\n",
    "                # aggiungo all'output il contenuto dello stack\n",
    "                output.append(stack.pop())\n",
    "            stack.pop()  # remove '('\n",
    "        else:  # aggiungi un termine all'outpuit\n",
    "            output.append(token)\n",
    "    while stack:  # svuota lo stack\n",
    "        output.append(stack.pop())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c0659bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_movie_description(\n",
    "    '../Code IR/data/movie.metadata.tsv', '../Code IR/data/plot_summaries.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "47838672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42204 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42204/42204 [00:04<00:00, 8917.93it/s]\n",
      "100%|██████████| 42204/42204 [00:35<00:00, 1172.88it/s]\n"
     ]
    }
   ],
   "source": [
    "ir = IrSystem.from_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "497473f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lord of the Flies]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir.phrase_query('speak during meetings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3caaeedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 161 179\n"
     ]
    }
   ],
   "source": [
    "print(len(ir.query('yoda')), len(ir.query(\n",
    "    'luke')), len(ir.query('wars')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "010076d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Afghan Luke,\n",
       " Daisy Town,\n",
       " Decoys 2: Alien Seduction,\n",
       " Out Cold,\n",
       " 2:37,\n",
       " Lilies of the Field,\n",
       " Scumbus,\n",
       " Death of a Gunfighter,\n",
       " Fatty and Mabel Adrift,\n",
       " Santa Baby,\n",
       " The Boys Club,\n",
       " SpaceCamp,\n",
       " Undiscovered,\n",
       " Fast Five,\n",
       " Star Wars Episode V: The Empire Strikes Back,\n",
       " Dual,\n",
       " Angels and Demons,\n",
       " Children of Men,\n",
       " Spiderhole,\n",
       " Spike and Suzy: The Texas Rangers,\n",
       " Children of the Corn V: Fields of Terror,\n",
       " Stagecoach,\n",
       " Animal Kingdom,\n",
       " The Prince of Tides,\n",
       " The Dukes of Hazzard: Reunion!,\n",
       " Vanishing on 7th Street,\n",
       " Green Light,\n",
       " Still Crazy,\n",
       " Coming Home,\n",
       " Decoys,\n",
       " Halloween Resurrection,\n",
       " Imaginationland Episode II,\n",
       " Slaves,\n",
       " Jennifer,\n",
       " Nagarangalil Chennu Raparkam,\n",
       " Star Wars Episode IV: A New Hope,\n",
       " Memphis Belle,\n",
       " Wishology,\n",
       " The Wendell Baker Story,\n",
       " The Little Troll Prince: A Christmas Parable,\n",
       " Mustang Country,\n",
       " Macon County Line,\n",
       " The Long Kiss Goodnight,\n",
       " The Dukes of Hazzard: Hazzard in Hollywood!,\n",
       " A Woman's Secret,\n",
       " No Name on the Bullet,\n",
       " Tanner on Tanner,\n",
       " The Toy that Saved Christmas,\n",
       " Troops,\n",
       " 13Hrs,\n",
       " Death Race 2,\n",
       " Zerophilia,\n",
       " Lego Star Wars: Bombad Bounty,\n",
       " The Stranger Within,\n",
       " Li'l Abner,\n",
       " King of the Coiners,\n",
       " I'm With Lucy,\n",
       " Renegade,\n",
       " The 5th Quarter,\n",
       " Sherlock Holmes,\n",
       " Thirteen,\n",
       " Teaching Mrs. Tingle,\n",
       " Undead or Alive,\n",
       " Silence Becomes You,\n",
       " The Last Supper,\n",
       " In the Name of Love: A Texas Tragedy,\n",
       " The Haunting,\n",
       " Tol'able David,\n",
       " The Frankenstein Brothers,\n",
       " Step Up 3-D,\n",
       " Eddie and the Cruisers II: Eddie Lives,\n",
       " Crossfire Trail,\n",
       " The Haunting,\n",
       " Something, Something, Something Dark Side,\n",
       " The Star Wars Holiday Special,\n",
       " On the Edge of Innocence,\n",
       " First Kid,\n",
       " The Witches,\n",
       " Manhattan Baby,\n",
       " Percy Jackson & the Olympians: The Lightning Thief,\n",
       " Dracula II Ascension,\n",
       " The Thorn Birds: The Missing Years,\n",
       " Cool Hand Luke,\n",
       " Garden of Evil,\n",
       " Ex,\n",
       " All Hat,\n",
       " The Lone Ranger,\n",
       " The Samaritan,\n",
       " The Three Musketeers,\n",
       " Slipstream,\n",
       " Strawberry Fields,\n",
       " Are You Ready for Love?,\n",
       " Count Three and Pray,\n",
       " Scooby-Doo! Camp Scare,\n",
       " Return of the Ewok,\n",
       " El Dorado,\n",
       " Immortals,\n",
       " A Charlie Brown Christmas,\n",
       " Ibunda,\n",
       " The Making of Star Wars,\n",
       " The Siege of Pinchgut,\n",
       " All Over Me,\n",
       " The Lawless Frontier,\n",
       " Cherrybomb,\n",
       " Gordy,\n",
       " Halloweentown,\n",
       " The Innkeepers,\n",
       " A Few Best Men,\n",
       " Fatty's Plucky Pup,\n",
       " The Skeleton Key,\n",
       " Where Love Has Gone,\n",
       " Star Wars Episode III: Revenge of the Sith,\n",
       " Lost and Delirious,\n",
       " Death Sentence,\n",
       " You Me and Captain Longbridge,\n",
       " Bullet in the Head,\n",
       " Dust,\n",
       " Wicker Park,\n",
       " Superman and the Mole Men,\n",
       " Red Dog,\n",
       " Silver Saddle,\n",
       " Stepmom,\n",
       " Used Cars,\n",
       " Star Wars Episode VI: Return of the Jedi,\n",
       " A Cinderella Story: Once Upon A Song,\n",
       " The Car,\n",
       " Repossessed,\n",
       " Dracula III Legacy,\n",
       " Kokoda,\n",
       " Halloweentown II: Kalabar's Revenge,\n",
       " South 5,\n",
       " Long Weekend,\n",
       " Prince Valiant,\n",
       " Confessions of a Shopaholic,\n",
       " Bloodshed,\n",
       " Condor, El,\n",
       " South of St. Louis,\n",
       " The Skulls,\n",
       " Stir of Echoes: The Homecoming,\n",
       " The Majestic,\n",
       " Professor Layton and the Eternal Diva,\n",
       " Mama, I Want to Sing!,\n",
       " Padatha Painkili,\n",
       " Night Watch,\n",
       " The Last Day of Summer,\n",
       " The Dukes of Hazzard,\n",
       " The Living End,\n",
       " Santa Baby 2,\n",
       " Merry Christmas, Drake & Josh,\n",
       " The Angels' Share,\n",
       " The Duel at Silver Creek,\n",
       " The Wackness,\n",
       " Suing the Devil,\n",
       " It's a Trap!,\n",
       " The Reef,\n",
       " Robot Chicken: Star Wars Episode II,\n",
       " Killer Movie,\n",
       " Hardwired,\n",
       " The Grind,\n",
       " Blitz,\n",
       " The Movie]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir.query('luke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fafb33ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Getting Even,\n",
       " Agent Vinod,\n",
       " Afghan Luke,\n",
       " Brothers,\n",
       " Charlie Wilson's War,\n",
       " The Veteran,\n",
       " Afghan Breakdown,\n",
       " Iron Man,\n",
       " Summer Heat,\n",
       " New Year's Eve,\n",
       " The Minion,\n",
       " The Storm,\n",
       " The Objective,\n",
       " Zombie Strippers,\n",
       " Outlaw,\n",
       " If I Should Fall,\n",
       " Dharmatma,\n",
       " The Christmas Card,\n",
       " The Boy Mir,\n",
       " The Hard Corps,\n",
       " Main Osama,\n",
       " The Whistleblower,\n",
       " Where in the World is Osama Bin Laden?,\n",
       " All Costs Paid,\n",
       " Kabul Express,\n",
       " 16 Days in Afghanistan,\n",
       " Armadillo,\n",
       " Fire Creek,\n",
       " Kabuliwala,\n",
       " Keerthi Chakra,\n",
       " Rambo,\n",
       " Aegan,\n",
       " 9th Company,\n",
       " Netaji Subhas Chandra Bose: The Forgotten Hero,\n",
       " Qayamat - A Love Triangle In Afghanistan,\n",
       " The Kite Runner,\n",
       " Beyond the Call,\n",
       " Stealing a Nation,\n",
       " Lions for Lambs,\n",
       " Afghan Massacre - the Convoy of Death,\n",
       " Afghan Muscles,\n",
       " Rambo III,\n",
       " Homeland Security,\n",
       " Kim,\n",
       " Savages,\n",
       " My Name is Khan,\n",
       " The Great Mouse Detective,\n",
       " Watchmen,\n",
       " Brothers,\n",
       " DC 9/11: Time of Crisis,\n",
       " Khamosh Pani,\n",
       " Fahrenheit 9/11,\n",
       " La Linea,\n",
       " Baran,\n",
       " Mission Istanbul,\n",
       " Enchantment,\n",
       " Osama,\n",
       " 25 Hill,\n",
       " The best Movie,\n",
       " Khuda Gawah,\n",
       " The Path to 9/11,\n",
       " The Beast,\n",
       " Batoru rowaiaru II: Chinkonka,\n",
       " Special Forces,\n",
       " Afghan Star,\n",
       " Hollow Man 2,\n",
       " Jerichow,\n",
       " New Jerusalem,\n",
       " The Power of Nightmares,\n",
       " Flight of Fury,\n",
       " Rethink Afghanistan,\n",
       " The War You Don't See,\n",
       " Source Code,\n",
       " Princess of Mars,\n",
       " Cargo 200,\n",
       " Breaking the Silence: Truth and Lies in the War on Terror,\n",
       " Royal Flash,\n",
       " Jodhaa Akbar,\n",
       " 31 North 62 East,\n",
       " Salmon Fishing in the Yemen,\n",
       " The Last Man on Planet Earth,\n",
       " Kabuliwala]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir.query('afghanistan')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
