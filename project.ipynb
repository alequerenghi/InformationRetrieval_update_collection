{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "785fc004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from BTrees.OOBTree import OOBTree\n",
    "from stop_words import get_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99126733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista dei postings aka i docID\n",
    "class PostingsList:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._postings_list = []\n",
    "\n",
    "    # crea una PostingsList da una lista di docID e la ordina (forse non necssario)\n",
    "    @classmethod\n",
    "    def from_postings_list(cls, postings_list: list[int]) -> 'PostingsList':\n",
    "        plist = cls()\n",
    "        postings_list.sort()\n",
    "        plist._postings_list = postings_list\n",
    "        return plist\n",
    "\n",
    "    # Crea una PostingsList da un singolo docID\n",
    "    @classmethod\n",
    "    def from_doc_id(cls, doc_id: int) -> 'PostingsList':\n",
    "        plist = cls()\n",
    "        plist._postings_list = [doc_id]\n",
    "        return plist\n",
    "\n",
    "    # Concatena due PostingsList. Le liste sono ordinate e i duplicati rimossi. other contiene una PostingsList creata successivamente a self (i docID saranno piu grandi o uguali)\n",
    "    def merge(self, other: \"PostingsList\") -> 'PostingsList':\n",
    "        if self._postings_list == []:\n",
    "            self._postings_list = other._postings_list\n",
    "        i = 0  # Start index for the other PostingList.\n",
    "        last = self._postings_list[-1]  # The last Posting in the current list.\n",
    "        # Loop through the other PostingList and skip duplicates.\n",
    "        while (i < len(other._postings_list) and last == other._postings_list[i]):\n",
    "            i += 1  # Increment the index if a duplicate is found.\n",
    "        # Append the non-duplicate postings from the other list.\n",
    "        self._postings_list += other._postings_list[i:]\n",
    "        return self\n",
    "\n",
    "    # Ottiene i titoli di documenti dai docID nella PostingsList\n",
    "    def get_from_corpus(self, corpus) -> list[str]:\n",
    "        return list(map(lambda x: corpus[x], self._postings_list))\n",
    "\n",
    "    # Effettua l'intersezione di due PostgingsList con il metodo del doppio indice\n",
    "    def intersection(self, other: \"PostingsList\") -> 'PostingsList':\n",
    "        plist = []\n",
    "        i = 0  # indice riferito a self\n",
    "        j = 0  # indice riferito a other\n",
    "        # finché non si eccede la dimensione di ciascuna lista:\n",
    "        while (i < len(self._postings_list)) and (j < len(other._postings_list)):\n",
    "            # se c'e' un match aggiungi l'elemento e incrmeneta entrambi\n",
    "            if self._postings_list[i] == other._postings_list[j]:\n",
    "                plist.append(self._postings_list[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            # altrimenti aumenta il piu piccolo dei due\n",
    "            elif self._postings_list[i] <= other._postings_list[j]:\n",
    "                i += 1\n",
    "            # altrimenti aumenta l'altro\n",
    "            else:\n",
    "                j += 1\n",
    "        return PostingsList.from_postings_list(plist)\n",
    "\n",
    "    # Effettua l'unione di due PostingsList con il metodo del doppio indice\n",
    "    def union(self, other: \"PostingsList\") -> 'PostingsList':\n",
    "        plist = []\n",
    "        i = 0  # indice riferito a self\n",
    "        j = 0  # indice riferito a other\n",
    "        # fintanto che gli indici sono piu' piccoli di entrambe le liste\n",
    "        while (i < len(self._postings_list)) and (j < len(other._postings_list)):\n",
    "            # aggiungi il docID e aumenta entrambi\n",
    "            if self._postings_list[i] == other._postings_list[j]:\n",
    "                plist.append(self._postings_list[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            # altrimenti aggiungi il docID e aumenta il piu' piccolo\n",
    "            elif self._postings_list[i] < other._postings_list[j]:\n",
    "                plist.append(self._postings_list[i])\n",
    "                i += 1\n",
    "            #  aggiungi l'altro e incrementalo\n",
    "            else:\n",
    "                plist.append(other._postings_list[j])\n",
    "                j += 1\n",
    "        # aggiungi la porzione restante di lista\n",
    "        if i < len(self._postings_list):  # nel caso in cui self era piu' lunga\n",
    "            plist += self._postings_list[i:]\n",
    "        elif j < len(other._postings_list):  # nel caso in cui other era piu' lunga\n",
    "            plist += other._postings_list[j:]\n",
    "        return PostingsList.from_postings_list(plist)\n",
    "\n",
    "    # Effettua la negazione del tipo AND NOT con il metodo dei due indici\n",
    "    def negation(self, other: 'PostingsList') -> 'PostingsList':\n",
    "        plist = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while (i < len(self._postings_list)) and (j < len(other._postings_list)):\n",
    "            # se self contiene il docID, scartalo e incrementa entrambi\n",
    "            if self._postings_list[i] == other._postings_list[j]:\n",
    "                i += 1\n",
    "                j += 1\n",
    "            # aggiungi il docID da self e incrementa se e' piu' piccolo\n",
    "            elif self._postings_list[i] < other._postings_list[j]:\n",
    "                plist.append(self._postings_list[i])\n",
    "                i += 1\n",
    "            # incrementa other\n",
    "            else:\n",
    "                j += 1\n",
    "        # aggiungi i documenti mancanti da self\n",
    "        if i < len(self._postings_list):  # se e' piu' lungo di other\n",
    "            plist += self._postings_list[i:]\n",
    "        return PostingsList.from_postings_list(plist)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \", \".join(map(str, self._postings_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54e4d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "074b34d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \"\"\"Remove punctuation and convert text to lowercase\"\"\"\n",
    "    return re.sub(r'[^\\w\\s^-]', '', text).lower()\n",
    "\n",
    "\n",
    "def tokenize(content) -> list:\n",
    "    \"\"\"Split normalized text into tokens\"\"\"\n",
    "    return normalize(content).split()\n",
    "\n",
    "\n",
    "class InvertedIndex:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.btree = OOBTree()  # usa un Btree per rendere piu' veloci aggiornamenti dell'indice\n",
    "\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, max_size=0) -> 'InvertedIndex':\n",
    "        terms = {}  # dizionario temporaneo per tenere l'indice iniziale\n",
    "        # per ogni documento\n",
    "        for doc_id, content in enumerate(tqdm(corpus, total=max_size or None)):\n",
    "            tokens_list = tokenize(content.description)\n",
    "            tokens_filtered = [t for t in tokens_list if t not in stop_words]\n",
    "            tokens = set(tokens_filtered)\n",
    "            # crea un set dei termini che contiene\n",
    "            #tokens = set(tokenize(content.description))\n",
    "            for token in tokens:  # per ogni termine\n",
    "                plist = PostingsList.from_doc_id(doc_id)\n",
    "                if token in terms:  # se contenuto\n",
    "                    terms[token].merge(plist)  # fai merge delle PostingsList\n",
    "                else:  # altrimenti aggiungi\n",
    "                    terms[token] = plist\n",
    "        idx = cls()\n",
    "        idx.btree.update(terms)\n",
    "        return idx\n",
    "\n",
    "    # crea il biword index per le phrase queries\n",
    "    @classmethod\n",
    "    def from_corpus_biword(cls, corpus, max_size=0) -> 'InvertedIndex':\n",
    "        terms = {}\n",
    "        # per ogni documento\n",
    "        for doc_id, content in enumerate(tqdm(corpus, total=max_size or None)):\n",
    "            tokens = tokenize(content.description)\n",
    "            # per ogni parola\n",
    "            for i in range(len(tokens) - 1):\n",
    "                biword = tokens[i]+tokens[i+1]\n",
    "                plist = PostingsList.from_doc_id(doc_id)\n",
    "                if biword in terms:\n",
    "                    terms[biword].merge(plist)\n",
    "                else:\n",
    "                    terms[biword] = plist\n",
    "        idx = cls()\n",
    "        idx.btree.update(terms)\n",
    "        return idx\n",
    "\n",
    "    def merge(self, other: 'InvertedIndex') -> 'InvertedIndex':\n",
    "        for term, postings in other.btree.items():\n",
    "            if term in self.btree:\n",
    "                self.btree[term].merge(postings)\n",
    "            else:\n",
    "                self.btree[term] = postings\n",
    "        return self\n",
    "\n",
    "    def __getitem__(self, key: str) -> PostingsList:\n",
    "        return self.btree[key]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.btree)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.btree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf68017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDescription:\n",
    "    def __init__(self, title: str, description: str) -> None:\n",
    "        self.title = title\n",
    "        self.description = description\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.title\n",
    "\n",
    "# leggi il file descrizione e metadata e crea un corpus (collection di documenti)\n",
    "\n",
    "\n",
    "def read_movie_description(movie_metadata, description_file) -> list[MovieDescription]:\n",
    "    names = {}\n",
    "    corpus = []\n",
    "    with open(movie_metadata, 'r') as file:  # leggi i metadati\n",
    "        movie_names = csv.reader(file, delimiter='\\t')\n",
    "        for description in movie_names:  # aggiungi a names la coppia id_film: titolo\n",
    "            names[description[0]] = description[2]\n",
    "    with open(description_file, 'r') as file:  # leggi le descrizioni\n",
    "        descriptions = csv.reader(file, delimiter='\\t')\n",
    "        for description in descriptions:\n",
    "            try:\n",
    "                # aggiungi al corpus il titolo e la descrizione di ciascun film\n",
    "                corpus.append(MovieDescription(\n",
    "                    # il docID e' la posizione del documento nel corpus\n",
    "                    names[description[0]], description[1]))\n",
    "            except KeyError:\n",
    "                pass\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9731a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrSystem:\n",
    "    def __init__(self, corpus: list[MovieDescription], index: InvertedIndex, biword: InvertedIndex, max_size_aux=10000) -> None:\n",
    "        self._corpus = corpus\n",
    "        self._index = index  # inverted index\n",
    "        self._invalid_vec = []  # invalidation bit vector\n",
    "        self._temp_idx = None  # indice ausiliario\n",
    "        self.max_size_aux = max_size_aux  # massimo docID assegnato\n",
    "        self._biword = biword  # inverted index con biword per phrase queries\n",
    "        self._temp_biword = None\n",
    "\n",
    "    # Crea l'indice e il biword\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus: list[MovieDescription]) -> 'IrSystem':\n",
    "        index = InvertedIndex.from_corpus(corpus)\n",
    "        biword = InvertedIndex.from_corpus_biword(corpus)\n",
    "        ir = cls(corpus, index, biword)\n",
    "        ir._invalid_vec = [0] * len(corpus)\n",
    "        return ir\n",
    "\n",
    "    # Segna documenti cancellati\n",
    "    def delete_docs(self, documents: list[int]) -> 'IrSystem':\n",
    "        for doc in documents:\n",
    "            self._invalid_vec[doc] = 1\n",
    "        return self\n",
    "\n",
    "    # Aggiungi documenti nuovi all'indice ausiliario\n",
    "    def add_docs(self, corpus: list[MovieDescription]) -> 'IrSystem':\n",
    "        # i nuovi documenti usano docID piu' grandi\n",
    "        aux = InvertedIndex.from_corpus(corpus, len(self._invalid_vec))\n",
    "        if self._temp_idx is None:  # se non e' presente nell'indice ausiliario\n",
    "            self._temp_idx = aux  # aggiungilo\n",
    "        else:  # altrimenti\n",
    "            self._temp_idx.merge(aux)\n",
    "        if len(self._temp_idx) > self.max_size_aux:  # se l'indice ausiliario e' troppo grande\n",
    "            self.merge_idx()  # fai merge\n",
    "\n",
    "        aux_biword = InvertedIndex.from_corpus_biword(corpus, len(self._invalid_vec))\n",
    "        if self._temp_biword is None:\n",
    "            self._temp_biword = aux_biword\n",
    "        else:\n",
    "            self._temp_biword.merge(aux_biword)\n",
    "        # aggiorna la dimensione massima attuale\n",
    "        self.max_size_aux += len(corpus)\n",
    "        # aggiorna l'invalidation bit vector\n",
    "        self._invalid_vec += [0] * len(corpus)\n",
    "        return self\n",
    "\n",
    "    # Merge dell'indice ausilario con l'InvertedIndex\n",
    "    def merge_idx(self) -> 'IrSystem':\n",
    "        self._index.merge(self._temp_idx)\n",
    "        self._biword.merge(self._temp_biword)\n",
    "        self._temp_idx = None\n",
    "        self._temp_biword = None\n",
    "        return self\n",
    "\n",
    "    # Effettua una query booleana combinando i termini con AND, OR e NOT\n",
    "    def query(self, query: str) -> list[str]:\n",
    "        tokens = query.split()\n",
    "        # riscrive la query con gli operatori postfix\n",
    "        postfix = infix_to_postfix(tokens)\n",
    "        stack = []  # PostingsList ancora da processare\n",
    "        for token in postfix:\n",
    "            if token in ('AND', 'OR', 'NOT'):\n",
    "                right = stack.pop()\n",
    "                left = stack.pop()\n",
    "                if token == 'AND':  # caso AND, conviene ottimizzare la query facendo l'intersezione delle liste piu' corte in primis\n",
    "                    if not isinstance(left, list):\n",
    "                        left = [left]\n",
    "                    if not isinstance(right, list):\n",
    "                        right = [right]\n",
    "                    # aggiungi allo stack una lista [left, right]\n",
    "                    stack.append(left + right)\n",
    "                elif token in ('OR', 'NOT'):\n",
    "                    if isinstance(left, list):  # se left e' una lista (catena di AND)\n",
    "                        # effettua la sequenza di AND\n",
    "                        left = self._optimize_and_query(left)\n",
    "                    if isinstance(right, list):  # se right e' una lista (catena di AND)\n",
    "                        # effettua la sequenza di AND\n",
    "                        right = self._optimize_and_query(right)\n",
    "                    if token == 'OR':  # effettua l'OR\n",
    "                        stack.append(left.union(right))\n",
    "                    else:  # effettua il NOT (AND NOT)\n",
    "                        stack.append(left.negation(right))\n",
    "            else:  # aggiungi una PostingsList da processare allo stack\n",
    "                base = self._index.btree.get(\n",
    "                    token, PostingsList.from_postings_list([]))\n",
    "                aux = self._temp_idx.btree.get(token, PostingsList.from_postings_list(\n",
    "                    [])) if self._temp_idx else PostingsList.from_postings_list([])\n",
    "                stack.append(base.merge(aux))\n",
    "        result = stack.pop()  # estrai l'ultimo elemento (risultato finale)\n",
    "        if isinstance(result, list):  # se e' tuttora una lista (= catena di AND), fai l'intersezione\n",
    "            result = self._optimize_and_query(result)\n",
    "        # elimina i documenti cancellati\n",
    "        return self._remove_deleted(result).get_from_corpus(self._corpus)\n",
    "\n",
    "    def _remove_deleted(self, result: PostingsList) -> PostingsList:\n",
    "        for doc_id, deleted in enumerate(self._invalid_vec):\n",
    "            if deleted and doc_id in result._postings_list:\n",
    "                result._postings_list.remove(doc_id)\n",
    "        return result\n",
    "\n",
    "    # Effettua operazioni di AND consecutive facendo l'intersezione di PostingsList piu' corte prima\n",
    "    def _optimize_and_query(self, terms: list[PostingsList]) -> PostingsList:\n",
    "        # ordina le PostingsList per lunghezza crescente\n",
    "        plist = sorted(terms, key=lambda x: len(x._postings_list))\n",
    "        result = reduce(lambda x, y: x.intersection(y), plist)\n",
    "        return result\n",
    "\n",
    "    # Ricerca una sequenza specifica di parola nel corpus con biword\n",
    "    def phrase_query(self, query: str) -> list[str]:\n",
    "        biword_query = []\n",
    "        words = query.split()\n",
    "        for i in range(len(words)-1):\n",
    "            # concatena le parole della query in coppie\n",
    "            biword_query.append(words[i]+words[i+1])\n",
    "        postings = []\n",
    "        # cerca le biword nel biword index\n",
    "        for biword in biword_query:\n",
    "            base = self._biword.btree.get(\n",
    "                biword, PostingsList.from_postings_list([]))\n",
    "            aux = self._temp_biword.btree.get(biword, PostingsList.from_postings_list(\n",
    "                [])) if self._temp_biword else PostingsList.from_postings_list([])\n",
    "            postings.append(base.merge(aux))\n",
    "        # effettua l'intersezione delle PostingsList trovate\n",
    "        plist = reduce(lambda x, y: x.intersection(y), postings)\n",
    "        # rimuovi cancellati e ritorna i risultati\n",
    "        return self._remove_deleted(plist).get_from_corpus(self._corpus)\n",
    "\n",
    "# Rende una espressione da infix a postfix: a AND b OR c -> a b AND c OR\n",
    "\n",
    "\n",
    "def infix_to_postfix(tokens: list[str]) -> list[str]:\n",
    "    output = []  # risultato finale\n",
    "    stack = []  # ancora da processare\n",
    "    for token in tokens:\n",
    "        if token in ('AND', 'OR', 'NOT'):  # se e' un operatore\n",
    "            # finche' ci sono parole da processare e non e' una parentesi\n",
    "            while (stack and stack[-1] != '('):\n",
    "                # aggiungi al risultato finale le parole una dopo l'altra\n",
    "                output.append(stack.pop())\n",
    "            stack.append(token)  # aggiungi l'operatore allo stack\n",
    "        elif token == '(':  # aggiungi la parentesi allo stack\n",
    "            stack.append(token)\n",
    "        elif token == ')':\n",
    "            # fino a che non incontro la parentesi aperta o si svuota lo stack\n",
    "            while stack and stack[-1] != '(':\n",
    "                # aggiungo all'output il contenuto dello stack\n",
    "                output.append(stack.pop())\n",
    "            stack.pop()  # remove '('\n",
    "        else:  # aggiungi un termine all'outpuit\n",
    "            output.append(token)\n",
    "    while stack:  # svuota lo stack\n",
    "        output.append(stack.pop())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0659bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_movie_description(\n",
    "    '../Code IR/data/movie.metadata.tsv', '../Code IR/data/plot_summaries.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47838672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42204/42204 [00:15<00:00, 2799.41it/s]\n",
      "100%|██████████| 42204/42204 [00:19<00:00, 2183.61it/s]\n"
     ]
    }
   ],
   "source": [
    "ir = IrSystem.from_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "497473f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lord of the Flies]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir.phrase_query('speak during meetings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78650a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Amici miei,\n",
       " Wiggle Time,\n",
       " Battling with Buffalo Bill,\n",
       " Dobrynya Nikitich and Zmey Gorynych,\n",
       " A Pleasant Journey,\n",
       " Narendra Makan Jayakanthan Vaka,\n",
       " La Tía Alejandra,\n",
       " Kasoor,\n",
       " Tarzan and the Leopard Woman,\n",
       " Anne of Green Gables,\n",
       " Vanakkam Thalaiva,\n",
       " Little Lili,\n",
       " Technotise Edit & I,\n",
       " Stalingrad,\n",
       " India: Kingdom of the Tiger,\n",
       " The Marshal of Windy Hollow,\n",
       " The Burning Train,\n",
       " Yavanika,\n",
       " Like a Dragon,\n",
       " Heartbreaker,\n",
       " Book Revue,\n",
       " Acid Factory,\n",
       " Bombay Talkie,\n",
       " The Butler's in Love,\n",
       " Вчера,\n",
       " Amen.,\n",
       " Libertas,\n",
       " Journey to the Beginning of Time,\n",
       " Blitz Wolf,\n",
       " Quest of the Delta Knights,\n",
       " Jakob the Liar,\n",
       " Return to Oz,\n",
       " Oh My God,\n",
       " Everyone's Hero,\n",
       " California Dreamin',\n",
       " Once Upon a Crime...,\n",
       " Mahaul Theek Hai,\n",
       " The Mahabharata,\n",
       " Blowup,\n",
       " Ghost Story,\n",
       " Fatal Termination,\n",
       " Vinayaka Chaviti,\n",
       " The Story of Osaka Castle,\n",
       " The Interpreter,\n",
       " The 5th Quarter,\n",
       " How High,\n",
       " Aayushkalam,\n",
       " Nights and Days,\n",
       " Land of Doom,\n",
       " How the Grinch Stole Christmas!,\n",
       " Paramanandayya Shishyula Katha,\n",
       " Sweeney 2,\n",
       " Totally Fucked Up,\n",
       " Nefertiti, figlia del sole,\n",
       " Raja Makutam,\n",
       " Diagnosis: Death,\n",
       " The Cabinet of Dr. Caligari,\n",
       " Return of Mr. Superman,\n",
       " Lady Godiva of Coventry,\n",
       " Arctic Blast,\n",
       " Deadline Auto Theft,\n",
       " Gorgeous,\n",
       " Soccer Dog: European Cup,\n",
       " Little Soldiers,\n",
       " The Racket,\n",
       " Pasión de gavilanes,\n",
       " Nizhalkuthu,\n",
       " The Water Margin,\n",
       " Kin-Dza-Dza,\n",
       " Ex-Lady,\n",
       " Blanche Fury,\n",
       " Lisbon Story,\n",
       " Believe,\n",
       " The Best Little Whorehouse in Texas,\n",
       " Laadam,\n",
       " Anbe Vaa,\n",
       " The Cabinet of Dr. Caligari,\n",
       " Polygon,\n",
       " Dreamscape,\n",
       " The Pope Must Die,\n",
       " Babes in Toyland,\n",
       " A Feather in His Hare,\n",
       " Tegan the Vegan,\n",
       " Cars 2,\n",
       " Arabesque,\n",
       " Abraham,\n",
       " Arunachalam,\n",
       " The Night Before Christmas,\n",
       " The Class of Nuke 'Em High 3: The Good, the Bad and the Subhumanoid,\n",
       " The Adventures of Mark Twain,\n",
       " Modhi Vilayadu,\n",
       " 29th Street,\n",
       " Colonel Blood,\n",
       " Blackball,\n",
       " Bhargavacharitham Moonam Khandam,\n",
       " Love Me Tender,\n",
       " Moss & Cat,\n",
       " Thaïs,\n",
       " National Lampoon's Senior Trip,\n",
       " Lumumba,\n",
       " Wake Me When The War Is Over,\n",
       " Married in Name Only,\n",
       " The Divine Woman,\n",
       " The Engagement Ring,\n",
       " Dracula,\n",
       " The World's Greatest Athlete,\n",
       " I Love to Singa,\n",
       " Afinidades,\n",
       " À gauche en sortant de l'ascenseur,\n",
       " The Chase,\n",
       " Viva Knievel!,\n",
       " How I Wonder What You Are,\n",
       " Gold Raiders,\n",
       " The Dognapper,\n",
       " Dead Man's Letters,\n",
       " Guys and Dolls,\n",
       " The Ghost of Slumber Mountain,\n",
       " Yamudiki Mogudu,\n",
       " Noriko's Dinner Table,\n",
       " Nate and Hayes,\n",
       " Be Cool,\n",
       " Tattoo]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir.phrase_query('the plot is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3caaeedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 161 179\n"
     ]
    }
   ],
   "source": [
    "print(len(ir.query('yoda')), len(ir.query(\n",
    "    'luke')), len(ir.query('wars')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "010076d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Afghan Luke,\n",
       " Daisy Town,\n",
       " Decoys 2: Alien Seduction,\n",
       " Out Cold,\n",
       " 2:37,\n",
       " Lilies of the Field,\n",
       " Scumbus,\n",
       " Death of a Gunfighter,\n",
       " Fatty and Mabel Adrift,\n",
       " Santa Baby,\n",
       " The Boys Club,\n",
       " SpaceCamp,\n",
       " Undiscovered,\n",
       " Fast Five,\n",
       " Star Wars Episode V: The Empire Strikes Back,\n",
       " Dual,\n",
       " Angels and Demons,\n",
       " Children of Men,\n",
       " Spiderhole,\n",
       " Spike and Suzy: The Texas Rangers,\n",
       " Children of the Corn V: Fields of Terror,\n",
       " Stagecoach,\n",
       " Animal Kingdom,\n",
       " The Prince of Tides,\n",
       " The Dukes of Hazzard: Reunion!,\n",
       " Vanishing on 7th Street,\n",
       " Green Light,\n",
       " Still Crazy,\n",
       " Coming Home,\n",
       " Decoys,\n",
       " Halloween Resurrection,\n",
       " Imaginationland Episode II,\n",
       " Slaves,\n",
       " Jennifer,\n",
       " Nagarangalil Chennu Raparkam,\n",
       " Star Wars Episode IV: A New Hope,\n",
       " Memphis Belle,\n",
       " Wishology,\n",
       " The Wendell Baker Story,\n",
       " The Little Troll Prince: A Christmas Parable,\n",
       " Mustang Country,\n",
       " Macon County Line,\n",
       " The Long Kiss Goodnight,\n",
       " The Dukes of Hazzard: Hazzard in Hollywood!,\n",
       " A Woman's Secret,\n",
       " No Name on the Bullet,\n",
       " Tanner on Tanner,\n",
       " The Toy that Saved Christmas,\n",
       " Troops,\n",
       " 13Hrs,\n",
       " Death Race 2,\n",
       " Zerophilia,\n",
       " Lego Star Wars: Bombad Bounty,\n",
       " The Stranger Within,\n",
       " Li'l Abner,\n",
       " King of the Coiners,\n",
       " I'm With Lucy,\n",
       " Renegade,\n",
       " The 5th Quarter,\n",
       " Sherlock Holmes,\n",
       " Thirteen,\n",
       " Teaching Mrs. Tingle,\n",
       " Undead or Alive,\n",
       " Silence Becomes You,\n",
       " The Last Supper,\n",
       " In the Name of Love: A Texas Tragedy,\n",
       " The Haunting,\n",
       " Tol'able David,\n",
       " The Frankenstein Brothers,\n",
       " Step Up 3-D,\n",
       " Eddie and the Cruisers II: Eddie Lives,\n",
       " Crossfire Trail,\n",
       " The Haunting,\n",
       " Something, Something, Something Dark Side,\n",
       " The Star Wars Holiday Special,\n",
       " On the Edge of Innocence,\n",
       " First Kid,\n",
       " The Witches,\n",
       " Manhattan Baby,\n",
       " Percy Jackson & the Olympians: The Lightning Thief,\n",
       " Dracula II Ascension,\n",
       " The Thorn Birds: The Missing Years,\n",
       " Cool Hand Luke,\n",
       " Garden of Evil,\n",
       " Ex,\n",
       " All Hat,\n",
       " The Lone Ranger,\n",
       " The Samaritan,\n",
       " The Three Musketeers,\n",
       " Slipstream,\n",
       " Strawberry Fields,\n",
       " Are You Ready for Love?,\n",
       " Count Three and Pray,\n",
       " Scooby-Doo! Camp Scare,\n",
       " Return of the Ewok,\n",
       " El Dorado,\n",
       " Immortals,\n",
       " A Charlie Brown Christmas,\n",
       " Ibunda,\n",
       " The Making of Star Wars,\n",
       " The Siege of Pinchgut,\n",
       " All Over Me,\n",
       " The Lawless Frontier,\n",
       " Cherrybomb,\n",
       " Gordy,\n",
       " Halloweentown,\n",
       " The Innkeepers,\n",
       " A Few Best Men,\n",
       " Fatty's Plucky Pup,\n",
       " The Skeleton Key,\n",
       " Where Love Has Gone,\n",
       " Star Wars Episode III: Revenge of the Sith,\n",
       " Lost and Delirious,\n",
       " Death Sentence,\n",
       " You Me and Captain Longbridge,\n",
       " Bullet in the Head,\n",
       " Dust,\n",
       " Wicker Park,\n",
       " Superman and the Mole Men,\n",
       " Red Dog,\n",
       " Silver Saddle,\n",
       " Stepmom,\n",
       " Used Cars,\n",
       " Star Wars Episode VI: Return of the Jedi,\n",
       " A Cinderella Story: Once Upon A Song,\n",
       " The Car,\n",
       " Repossessed,\n",
       " Dracula III Legacy,\n",
       " Kokoda,\n",
       " Halloweentown II: Kalabar's Revenge,\n",
       " South 5,\n",
       " Long Weekend,\n",
       " Prince Valiant,\n",
       " Confessions of a Shopaholic,\n",
       " Bloodshed,\n",
       " Condor, El,\n",
       " South of St. Louis,\n",
       " The Skulls,\n",
       " Stir of Echoes: The Homecoming,\n",
       " The Majestic,\n",
       " Professor Layton and the Eternal Diva,\n",
       " Mama, I Want to Sing!,\n",
       " Padatha Painkili,\n",
       " Night Watch,\n",
       " The Last Day of Summer,\n",
       " The Dukes of Hazzard,\n",
       " The Living End,\n",
       " Santa Baby 2,\n",
       " Merry Christmas, Drake & Josh,\n",
       " The Angels' Share,\n",
       " The Duel at Silver Creek,\n",
       " The Wackness,\n",
       " Suing the Devil,\n",
       " It's a Trap!,\n",
       " The Reef,\n",
       " Robot Chicken: Star Wars Episode II,\n",
       " Killer Movie,\n",
       " Hardwired,\n",
       " The Grind,\n",
       " Blitz,\n",
       " The Movie]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir.query('luke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fafb33ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mabout\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mIrSystem.query\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m     85\u001b[39m         base = \u001b[38;5;28mself\u001b[39m._index.btree.get(\n\u001b[32m     86\u001b[39m             token, PostingsList.from_postings_list([]))\n\u001b[32m     87\u001b[39m         aux = \u001b[38;5;28mself\u001b[39m._temp_idx.btree.get(token, PostingsList.from_postings_list(\n\u001b[32m     88\u001b[39m             [])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._temp_idx \u001b[38;5;28;01melse\u001b[39;00m PostingsList.from_postings_list([])\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m         stack.append(\u001b[43mbase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43maux\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     90\u001b[39m result = stack.pop()  \u001b[38;5;66;03m# estrai l'ultimo elemento (risultato finale)\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;66;03m# se e' tuttora una lista (= catena di AND), fai l'intersezione\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mPostingsList.merge\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mself\u001b[39m._postings_list = other._postings_list\n\u001b[32m     26\u001b[39m i = \u001b[32m0\u001b[39m  \u001b[38;5;66;03m# Start index for the other PostingList.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m last = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_postings_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# The last Posting in the current list.\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Loop through the other PostingList and skip duplicates.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m (i < \u001b[38;5;28mlen\u001b[39m(other._postings_list) \u001b[38;5;129;01mand\u001b[39;00m last == other._postings_list[i]):\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "ir.query('about')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)  # per replicabilità\n",
    "random.shuffle(corpus)\n",
    "\n",
    "n = len(corpus)\n",
    "third = n // 3\n",
    "\n",
    "part1 = corpus[:third]\n",
    "part2 = corpus[third:2*third]\n",
    "part3 = corpus[2*third:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe7a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28136/28136 [00:02<00:00, 10707.06it/s]\n",
      "100%|██████████| 28136/28136 [00:15<00:00, 1780.43it/s]\n",
      "100%|██████████| 14068/14068 [00:01<00:00, 10731.56it/s]\n",
      "100%|██████████| 14068/14068 [00:02<00:00, 4876.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Barbarella,\n",
       " Meet Dave,\n",
       " Voyage of the Rock Aliens,\n",
       " Critters 2: The Main Course,\n",
       " Jimmy Neutron: Boy Genius,\n",
       " My Favorite Martian,\n",
       " AVPR: Aliens vs Predator - Requiem,\n",
       " Cosmos: War of the Planets,\n",
       " Bloodbath at the House of Death,\n",
       " Dreamcatcher,\n",
       " Cowboys & Aliens,\n",
       " Dragon Ball Z: Broly – The Legendary Super Saiyan,\n",
       " Lifeforce,\n",
       " Otaku no Video,\n",
       " Enemy Mine,\n",
       " Abbott and Costello Go to Mars,\n",
       " Alien,\n",
       " Things to Come,\n",
       " The Hasty Hare,\n",
       " Invasion of the Saucer Men,\n",
       " Little Alvin and the Mini-Munks,\n",
       " Alice's Birthday,\n",
       " Cowboy Bebop: The Movie,\n",
       " Gamera vs. Zigra,\n",
       " Contact,\n",
       " Solar Crisis,\n",
       " Dragon Ball Z: The Return of Cooler,\n",
       " Odin: Photon Sailer Starlight,\n",
       " Gorath,\n",
       " Robinson Crusoe on Mars,\n",
       " Koi... Mil Gaya,\n",
       " Star Wreck: In the Pirkinning,\n",
       " Project Moonbase,\n",
       " Conquest of Space,\n",
       " Laserblast,\n",
       " Team America: World Police,\n",
       " Megazone 23,\n",
       " Casper Meets Wendy,\n",
       " Planet of Dinosaurs,\n",
       " Zone Troopers,\n",
       " The Terrornauts,\n",
       " Duck Dodgers and the Return of the 24½th Century,\n",
       " WALL-E,\n",
       " World Without End,\n",
       " Galaxy Quest,\n",
       " Men in Black 3,\n",
       " On the Silver Globe,\n",
       " Moscow-Cassiopeia,\n",
       " The Alien,\n",
       " Spaceballs,\n",
       " Queen of Blood,\n",
       " Superman IV: The Quest For Peace,\n",
       " E.T. the Extra-Terrestrial,\n",
       " The Sheriff and the Satellite Kid,\n",
       " Earth vs. the Flying Saucers,\n",
       " Blackbirds at Bangpleng,\n",
       " The Black Hole,\n",
       " Science Ninja Team Gatchaman: The Movie,\n",
       " The Pickle,\n",
       " Yamato Takeru,\n",
       " Acción mutante,\n",
       " Race to Witch Mountain,\n",
       " Planeta Bur,\n",
       " Invasion of the Star Creatures,\n",
       " Planet of the Vampires,\n",
       " Negadon: The Monster from Mars,\n",
       " 2001: A Space Odyssey,\n",
       " Sunshine,\n",
       " The Strange World of Planet X,\n",
       " The Creeping Terror,\n",
       " Daleks - Invasion Earth 2150 AD,\n",
       " Alien from the Darkness,\n",
       " Treasure Planet,\n",
       " I Married a Monster from Outer Space,\n",
       " Boldly Going Nowhere,\n",
       " Yogi Bear and the Magical Flight of the Spruce Goose,\n",
       " Animal Crossing,\n",
       " Alien Trespass,\n",
       " Aliens in the Wild, Wild West,\n",
       " Rainbow Brite and the Star Stealer,\n",
       " The Garbage Pail Kids Movie,\n",
       " First Men in the Moon,\n",
       " Cat-Women of the Moon,\n",
       " Predators,\n",
       " Future War,\n",
       " Dragon Ball Z: The Tree of Might,\n",
       " Spaceflight IC-1,\n",
       " From the Earth to the Moon,\n",
       " Papelucho and the Martian,\n",
       " Ikarie XB-1,\n",
       " Monsters vs. Aliens,\n",
       " Abdullajon,\n",
       " Mars Needs Moms,\n",
       " Doraemon: The New Record of Nobita: Spaceblazer,\n",
       " Goke, Body Snatcher from Hell,\n",
       " The Man from Planet X,\n",
       " Keroro Gunso the Super Movie 3: Keroro vs. Keroro Great Sky Duel,\n",
       " Superman: Doomsday,\n",
       " The Day the Earth Stood Still,\n",
       " Earthbound,\n",
       " Iron Sky,\n",
       " The Doomsday Machine,\n",
       " Kremmen: The Movie,\n",
       " The Magic Portal,\n",
       " Slapstick of Another Kind,\n",
       " Returner,\n",
       " Tamala 2010: A Punk Cat in Space,\n",
       " The Man Who Fell to Earth,\n",
       " Hardware Wars,\n",
       " The Time Travelers,\n",
       " Lisztomania,\n",
       " Morons from Outer Space]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_base = IrSystem.from_corpus(part1 + part2)\n",
    "index_update = IrSystem.from_corpus(part3)\n",
    "\n",
    "#index_base.merge(index_update)\n",
    "\n",
    "index_base.query('spaceship')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
