{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "785fc004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import total_ordering, reduce\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from BTrees.OOBTree import OOBTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99126733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista dei postings aka i docID\n",
    "class PostingsList:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._postings_list = []\n",
    "\n",
    "    # crea una PostingsList da una lista di docID e la ordina (forse non necssario)\n",
    "    @classmethod\n",
    "    def from_postings_list(cls, postings_list: list[int]) -> 'PostingsList':\n",
    "        plist = cls()\n",
    "        postings_list.sort()\n",
    "        plist._postings_list = postings_list\n",
    "        return plist\n",
    "\n",
    "    # Crea una PostingsList da un singolo docID\n",
    "    @classmethod\n",
    "    def from_doc_id(cls, doc_id: int):\n",
    "        plist = cls()\n",
    "        plist._postings_list = [doc_id]\n",
    "        return plist\n",
    "\n",
    "    # Concatena due PostingsList. Le liste sono ordinate e i duplicati rimossi. other contiene una PostingsList creata successivamente a self (i docID saranno piu grandi o uguali)\n",
    "    def merge(self, other: \"PostingsList\") -> 'PostingsList':\n",
    "        i = 0  # Start index for the other PostingList.\n",
    "        last = self._postings_list[-1]  # The last Posting in the current list.\n",
    "        # Loop through the other PostingList and skip duplicates.\n",
    "        while (i < len(other._postings_list) and last == other._postings_list[i]):\n",
    "            i += 1  # Increment the index if a duplicate is found.\n",
    "        # Append the non-duplicate postings from the other list.\n",
    "        self._postings_list += other._postings_list[i:]\n",
    "        return self\n",
    "\n",
    "    # Ottiene i titoli di documenti dai docID nella PostingsList\n",
    "    def get_from_corpus(self, corpus):\n",
    "        return list(map(lambda x: corpus[x], self._postings_list))\n",
    "\n",
    "    # Effettua l'intersezione di due PostgingsList con il metodo del doppio indice\n",
    "    def intersection(self, other: \"PostingsList\") -> 'PostingsList':\n",
    "        plist = []\n",
    "        i = 0  # indice riferito a self\n",
    "        j = 0  # indice riferito a other\n",
    "        # finch√© non si eccede la dimensione di ciascuna lista:\n",
    "        while (i < len(self._postings_list)) and (j < len(other._postings_list)):\n",
    "            # se c'e' un match aggiungi l'elemento e incrmeneta entrambi\n",
    "            if self._postings_list[i] == other._postings_list[j]:\n",
    "                plist.append(self._postings_list[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            # altrimenti aumenta il piu piccolo dei due\n",
    "            elif self._postings_list[i] <= other._postings_list[j]:\n",
    "                i += 1\n",
    "            # altrimenti aumenta l'altro\n",
    "            else:\n",
    "                j += 1\n",
    "        return PostingsList.from_postings_list(plist)\n",
    "\n",
    "    # Effettua l'unione di due PostingsList con il metodo del doppio indice\n",
    "    def union(self, other: \"PostingsList\") -> 'PostingsList':\n",
    "        plist = []\n",
    "        i = 0  # indice riferito a self\n",
    "        j = 0  # indice riferito a other\n",
    "        # fintanto che gli indici sono piu' piccoli di entrambe le liste\n",
    "        while (i < len(self._postings_list)) and (j < len(other._postings_list)):\n",
    "            # aggiungi il docID e aumenta entrambi\n",
    "            if self._postings_list[i] == other._postings_list[j]:\n",
    "                plist.append(self._postings_list[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            # altrimenti aggiungi il docID e aumenta il piu' piccolo\n",
    "            elif self._postings_list[i] < other._postings_list[j]:\n",
    "                plist.append(self._postings_list[i])\n",
    "                i += 1\n",
    "            #  aggiungi l'altro e incrementalo\n",
    "            else:\n",
    "                plist.append(other._postings_list[j])\n",
    "                j += 1\n",
    "        # aggiungi la porzione restante di lista\n",
    "        if i < len(self._postings_list):  # nel caso in cui self era piu' lunga\n",
    "            plist += self._postings_list[i:]\n",
    "        elif j < len(other._postings_list):  # nel caso in cui other era piu' lunga\n",
    "            plist += other._postings_list[j:]\n",
    "        return PostingsList.from_postings_list(plist)\n",
    "\n",
    "    # Effettua la negazione del tipo AND NOT con il metodo dei due indici\n",
    "    def negation(self, other: 'PostingsList') -> 'PostingsList':\n",
    "        plist = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while (i < len(self._postings_list)) and (j < len(other._postings_list)):\n",
    "            # se self contiene il docID, scartalo e incrementa entrambi\n",
    "            if self._postings_list[i] == other._postings_list[j]:\n",
    "                i += 1\n",
    "                j += 1\n",
    "            # aggiungi il docID da self e incrementa se e' piu' piccolo\n",
    "            elif self._postings_list[i] < other._postings_list[j]:\n",
    "                plist.append(self._postings_list[i])\n",
    "                i += 1\n",
    "            # incrementa other\n",
    "            else:\n",
    "                j += 1\n",
    "        # aggiungi i documenti mancanti da self\n",
    "        if i < len(self._postings_list):  # se e' piu' lungo di other\n",
    "            plist += self._postings_list[i:]\n",
    "        return PostingsList.from_postings_list(plist)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \", \".join(map(str, self._postings_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "260f4b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plist = PostingsList.from_postings_list([1, 2])\n",
    "plist.negation(PostingsList.from_postings_list([1, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6263f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImpossibleMergeException(Exception):\n",
    "    pass\n",
    "\n",
    "# Da eliminare imo\n",
    "\n",
    "\n",
    "@total_ordering\n",
    "class Term:\n",
    "    def __init__(self, term: str, doc_id: int) -> None:\n",
    "        self.term = term\n",
    "        self.postings_list = PostingsList.from_doc_id(doc_id)\n",
    "\n",
    "    def merge(self, other: \"Term\") -> 'Term':\n",
    "        if self == other:\n",
    "            self.postings_list.merge(other.postings_list)\n",
    "        else:\n",
    "            raise ImpossibleMergeException\n",
    "        return self\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        return self.term == other.term\n",
    "\n",
    "    def __gt__(self, other) -> bool:\n",
    "        return self.term > other.term\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.term + \": \" + str(self.postings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b34d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    # Removes punctuation from the text using a regular expression.\n",
    "    no_punctuation = re.sub(r'[^\\w\\s^-]', '', text)\n",
    "    # Converts the text to lowercase.\n",
    "    downcase = no_punctuation.lower()\n",
    "    # Returns the normalized text.\n",
    "    return downcase\n",
    "\n",
    "\n",
    "def tokenize(content) -> list:\n",
    "    normalized = normalize(content)\n",
    "    return normalized.split()\n",
    "\n",
    "\n",
    "class InvertedIndex:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.btree = OOBTree()  # usa un Btree per rendere piu' veloci aggiornamenti dell'indice\n",
    "\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus, max_size=0):\n",
    "        terms = {}  # dizionario temporaneo per tenere l'indice iniziale\n",
    "        # per ogni documento\n",
    "        for doc_id, content in enumerate(tqdm(corpus), max_size):\n",
    "            # crea un set dei termini che contiene\n",
    "            tokens = set(tokenize(content.description))\n",
    "            for token in tokens:  # per ogni termine\n",
    "                if token in terms:  # se contenuto\n",
    "                    terms[token].merge(PostingsList.from_doc_id(\n",
    "                        doc_id))  # fai merge delle PostingsList\n",
    "                else:  # altrimenti aggiungi\n",
    "                    terms[token] = PostingsList.from_doc_id(doc_id)\n",
    "        idx = cls()\n",
    "        idx.btree.update(terms)\n",
    "        return idx\n",
    "\n",
    "    # crea il biword index per le phrase queries\n",
    "    @classmethod\n",
    "    def from_corpus_biword(cls, corpus, max_size=0):\n",
    "        terms = {}\n",
    "        # per ogni documento\n",
    "        for doc_id, content in enumerate(tqdm(corpus), max_size):\n",
    "            tokens = tokenize(content.description)\n",
    "            # per ogni parola\n",
    "            for i in range(1, len(tokens)-1):\n",
    "                token = tokens[i-1]+tokens[i]\n",
    "                if token in terms:\n",
    "                    terms[token].merge(PostingsList.from_doc_id(doc_id))\n",
    "                else:\n",
    "                    terms[token] = PostingsList.from_doc_id(doc_id)\n",
    "        idx = cls()\n",
    "        idx.btree.update(terms)\n",
    "        return idx\n",
    "\n",
    "    def __getitem__(self, key: str) -> PostingsList:\n",
    "        return self.btree[key]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.btree)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.btree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDescription:\n",
    "    def __init__(self, title: str, description: str):\n",
    "        self.title = title\n",
    "        self.description = description\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.title\n",
    "\n",
    "# leggi il file descrizione e metadata e crea un corpus (collection di documenti)\n",
    "\n",
    "\n",
    "def read_movie_description(movie_metadata, description_file):\n",
    "    names = {}\n",
    "    corpus = []\n",
    "    with open(movie_metadata, 'r') as file:  # leggi i metadati\n",
    "        movie_names = csv.reader(file, delimiter='\\t')\n",
    "        for description in movie_names:  # aggiungi a names la coppia id_film: titolo\n",
    "            names[description[0]] = description[2]\n",
    "    with open(description_file, 'r') as file:  # leggi le descrizioni\n",
    "        descriptions = csv.reader(file, delimiter='\\t')\n",
    "        for description in descriptions:\n",
    "            try:\n",
    "                # aggiungi al corpus il titolo e la descrizione di ciascun film\n",
    "                corpus.append(MovieDescription(\n",
    "                    # il docID e' la posizione del documento nel corpus\n",
    "                    names[description[0]], description[1]))\n",
    "            except KeyError:\n",
    "                pass\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9731a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrSystem:\n",
    "    def __init__(self, corpus: list[MovieDescription], index: InvertedIndex, biword: InvertedIndex, max_size_aux=10000) -> None:\n",
    "        self._corpus = corpus\n",
    "        self._index = index  # inverted index\n",
    "        self._invalid_vec = []  # invalidation bit vector\n",
    "        self._temp_idx = None  # indice ausiliario\n",
    "        self.max_size_aux = max_size_aux  # massimo docID assegnato\n",
    "        self._biword = biword  # inverted index con biword per phrase queries\n",
    "\n",
    "    # Crea l'indice e il biword\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus: list[MovieDescription]) -> 'IrSystem':\n",
    "        index = InvertedIndex.from_corpus(corpus)\n",
    "        biword = InvertedIndex.from_corpus_biword(corpus)\n",
    "        ir = cls(corpus, index, biword)\n",
    "        ir._invalid_vec = [0] * len(corpus)\n",
    "        return ir\n",
    "\n",
    "    # Segna documenti cancellati\n",
    "    def delete_docs(self, documents):\n",
    "        for doc in documents:\n",
    "            self._invalid_vec[doc] = 1\n",
    "\n",
    "    # Aggiungi documenti nuovi all'indice ausiliario\n",
    "    def add_docs(self, corpus):\n",
    "        # i nuovi documenti usano docID piu' grandi\n",
    "        aux = InvertedIndex.from_corpus(corpus, len(self._invalid_vec))\n",
    "        if self._temp_idx is None:  # se non e' presente nell'indice ausiliario\n",
    "            self._temp_idx = aux  # aggiungilo\n",
    "        else:  # altrimenti\n",
    "            pass  # fai il merge delle PostingsList\n",
    "        if len(self._temp_idx) > self.max_size_aux:  # se l'indice ausiliario e' troppo grande\n",
    "            self.merge_idx()  # fai merge\n",
    "        # aggiorna la dimensione massima attuale\n",
    "        self.max_size_aux += len(corpus)\n",
    "        # aggiorna l'invalidation bit vector\n",
    "        self._invalid_vec += [0] * len(corpus)\n",
    "\n",
    "    # Merge dell'indice ausilario con l'InvertedIndex\n",
    "    def merge_idx(self):\n",
    "        pass\n",
    "\n",
    "    # Effettua una query booleana combinando i termini con AND, OR e NOT\n",
    "    def query(self, query: str):\n",
    "        tokens = query.split()\n",
    "        # riscrive la query con gli operatori postfix\n",
    "        postfix = infix_to_postfix(tokens)\n",
    "        stack = []  # PostingsList ancora da processare\n",
    "        for token in postfix:\n",
    "            left = stack.pop()\n",
    "            right = stack.pop()\n",
    "            if token == 'AND':  # caso AND, conviene ottimizzare la query facendo l'intersezione delle liste piu' corte in primis\n",
    "                if not isinstance(left, list):\n",
    "                    left = [left]\n",
    "                if not isinstance(right, list):\n",
    "                    right = [right]\n",
    "                # aggiungi allo stack una lista [left, right]\n",
    "                stack.append(left + right)\n",
    "            elif token in ('OR', 'NOT'):\n",
    "                if isinstance(left, list):  # se left e' una lista (catena di AND)\n",
    "                    # effettua la sequenza di AND\n",
    "                    left = self.optimize_and_query(left)\n",
    "                if isinstance(right, list):  # se right e' una lista (catena di AND)\n",
    "                    # effettua la sequenza di AND\n",
    "                    right = self.optimize_and_query(right)\n",
    "                if token == 'OR':  # effettua l'OR\n",
    "                    stack.append(left.union(right))\n",
    "                else:  # effettua il NOT (AND NOT)\n",
    "                    # siccome lo stack contiene gli operandi in ordine invertito si invertono left e right\n",
    "                    stack.append(right.negation(left))\n",
    "            else:  # aggiungi una PostingsList da processare allo stack\n",
    "                try:  # prova a cercarla nell'InvertedIndex e in quello ausiliare\n",
    "                    stack.append(self._index[token].merge(\n",
    "                        self._temp_idx[token]))\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        result = stack.pop()  # estrai l'ultimo elemento (risultato finale)\n",
    "        if isinstance(result, list):  # se e' tuttora una lista (= catena di AND), fai l'intersezione\n",
    "            result = self.optimize_and_query(result)\n",
    "        for deleted in self._invalid_vec:  # elimina i documenti cancellati\n",
    "            if deleted:\n",
    "                result._postings_list.remove(deleted)\n",
    "        return result.get_from_corpus(self._corpus)\n",
    "\n",
    "    # Effettua operazioni di AND consecutive facendo l'intersezione di PostingsList piu' corte prima\n",
    "    def optimize_and_query(self, terms: list[PostingsList]):\n",
    "        # ordina le PostingsList per lunghezza crescente\n",
    "        plist = sorted(terms, key=lambda x: len(x._postings_list))\n",
    "        result = reduce(lambda x, y: x.intersection(y), plist)\n",
    "        return result\n",
    "\n",
    "    # Ricerca una sequenza specifica di parola nel corpus con biword\n",
    "    def phrase_query(self, query: str):\n",
    "        biword_query = []\n",
    "        words = query.split()\n",
    "        for i in range(1, len(words)-1):\n",
    "            # concatena le parole della query in coppie\n",
    "            biword_query.append(words[i-1]+words[i])\n",
    "        # cerca le biword nel biword index\n",
    "        postings = map(lambda w: self._biword[w], biword_query)\n",
    "        # effettua l'intersezione delle PostingsList trovate\n",
    "        plist = reduce(lambda x, y: x.intersection(y), postings)\n",
    "        return plist.get_from_corpus(self._corpus)\n",
    "\n",
    "# Rende una espressione da infix a postfix: a AND b OR c -> a b AND c OR\n",
    "\n",
    "\n",
    "def infix_to_postfix(tokens):\n",
    "    output = []  # risultato finale\n",
    "    stack = []  # ancora da processare\n",
    "    for token in tokens:\n",
    "        if token in ('AND', 'OR', 'NOT'):  # se e' un operatore\n",
    "            # finche' ci sono parole da processare e non e' una parentesi\n",
    "            while (stack and stack[-1] != '('):\n",
    "                # aggiungi al risultato finale le parole una dopo l'altra\n",
    "                output.append(stack.pop())\n",
    "            stack.append(token)  # aggiungi l'operatore allo stack\n",
    "        elif token == '(':  # aggiungi la parentesi allo stack\n",
    "            stack.append(token)\n",
    "        elif token == ')':\n",
    "            # fino a che non incontro la parentesi aperta o si svuota lo stack\n",
    "            while stack and stack[-1] != '(':\n",
    "                # aggiungo all'output il contenuto dello stack\n",
    "                output.append(stack.pop())\n",
    "            stack.pop()  # remove '('\n",
    "        else:  # aggiungi un termine all'outpuit\n",
    "            output.append(token)\n",
    "    while stack:  # svuota lo stack\n",
    "        output.append(stack.pop())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0659bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_movie_description(\n",
    "    '../Code IR/data/movie.metadata.tsv', '../Code IR/data/plot_summaries.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "47838672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42204/42204 [00:08<00:00, 4691.77it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42204/42204 [00:21<00:00, 1939.76it/s]\n"
     ]
    }
   ],
   "source": [
    "ir = IrSystem.from_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "497473f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The Neighbour No. 13, Lord of the Flies, Andha Naal, The Lover, Star Runners]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir.phrase_query('speak during meetings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caaeedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 161 179\n"
     ]
    }
   ],
   "source": [
    "print(len(ir.query('yoda')), len(ir.query(\n",
    "    'luke')), len(ir.query('wars')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010076d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Afghan Luke,\n",
       " Daisy Town,\n",
       " Decoys 2: Alien Seduction,\n",
       " Out Cold,\n",
       " 2:37,\n",
       " Lilies of the Field,\n",
       " Scumbus,\n",
       " Death of a Gunfighter,\n",
       " Fatty and Mabel Adrift,\n",
       " Santa Baby,\n",
       " The Boys Club,\n",
       " SpaceCamp,\n",
       " Undiscovered,\n",
       " Fast Five,\n",
       " Star Wars Episode V: The Empire Strikes Back,\n",
       " Dual,\n",
       " Angels and Demons,\n",
       " Children of Men,\n",
       " Spiderhole,\n",
       " Spike and Suzy: The Texas Rangers,\n",
       " Children of the Corn V: Fields of Terror,\n",
       " Stagecoach,\n",
       " Animal Kingdom,\n",
       " The Prince of Tides,\n",
       " The Dukes of Hazzard: Reunion!,\n",
       " Vanishing on 7th Street,\n",
       " Green Light,\n",
       " Still Crazy,\n",
       " Coming Home,\n",
       " Decoys,\n",
       " Halloween Resurrection,\n",
       " Imaginationland Episode II,\n",
       " Slaves,\n",
       " Jennifer,\n",
       " Nagarangalil Chennu Raparkam,\n",
       " Star Wars Episode IV: A New Hope,\n",
       " Memphis Belle,\n",
       " Wishology,\n",
       " The Wendell Baker Story,\n",
       " The Little Troll Prince: A Christmas Parable,\n",
       " Mustang Country,\n",
       " Macon County Line,\n",
       " The Long Kiss Goodnight,\n",
       " The Dukes of Hazzard: Hazzard in Hollywood!,\n",
       " A Woman's Secret,\n",
       " No Name on the Bullet,\n",
       " Tanner on Tanner,\n",
       " The Toy that Saved Christmas,\n",
       " Troops,\n",
       " 13Hrs,\n",
       " Death Race 2,\n",
       " Zerophilia,\n",
       " Lego Star Wars: Bombad Bounty,\n",
       " The Stranger Within,\n",
       " Li'l Abner,\n",
       " King of the Coiners,\n",
       " I'm With Lucy,\n",
       " Renegade,\n",
       " The 5th Quarter,\n",
       " Sherlock Holmes,\n",
       " Thirteen,\n",
       " Teaching Mrs. Tingle,\n",
       " Undead or Alive,\n",
       " Silence Becomes You,\n",
       " The Last Supper,\n",
       " In the Name of Love: A Texas Tragedy,\n",
       " The Haunting,\n",
       " Tol'able David,\n",
       " The Frankenstein Brothers,\n",
       " Step Up 3-D,\n",
       " Eddie and the Cruisers II: Eddie Lives,\n",
       " Crossfire Trail,\n",
       " The Haunting,\n",
       " Something, Something, Something Dark Side,\n",
       " The Star Wars Holiday Special,\n",
       " On the Edge of Innocence,\n",
       " First Kid,\n",
       " The Witches,\n",
       " Manhattan Baby,\n",
       " Percy Jackson & the Olympians: The Lightning Thief,\n",
       " Dracula II Ascension,\n",
       " The Thorn Birds: The Missing Years,\n",
       " Cool Hand Luke,\n",
       " Garden of Evil,\n",
       " Ex,\n",
       " All Hat,\n",
       " The Lone Ranger,\n",
       " The Samaritan,\n",
       " The Three Musketeers,\n",
       " Slipstream,\n",
       " Strawberry Fields,\n",
       " Are You Ready for Love?,\n",
       " Count Three and Pray,\n",
       " Scooby-Doo! Camp Scare,\n",
       " Return of the Ewok,\n",
       " El Dorado,\n",
       " Immortals,\n",
       " A Charlie Brown Christmas,\n",
       " Ibunda,\n",
       " The Making of Star Wars,\n",
       " The Siege of Pinchgut,\n",
       " All Over Me,\n",
       " The Lawless Frontier,\n",
       " Cherrybomb,\n",
       " Gordy,\n",
       " Halloweentown,\n",
       " The Innkeepers,\n",
       " A Few Best Men,\n",
       " Fatty's Plucky Pup,\n",
       " The Skeleton Key,\n",
       " Where Love Has Gone,\n",
       " Star Wars Episode III: Revenge of the Sith,\n",
       " Lost and Delirious,\n",
       " Death Sentence,\n",
       " You Me and Captain Longbridge,\n",
       " Bullet in the Head,\n",
       " Dust,\n",
       " Wicker Park,\n",
       " Superman and the Mole Men,\n",
       " Red Dog,\n",
       " Silver Saddle,\n",
       " Stepmom,\n",
       " Used Cars,\n",
       " Star Wars Episode VI: Return of the Jedi,\n",
       " A Cinderella Story: Once Upon A Song,\n",
       " The Car,\n",
       " Repossessed,\n",
       " Dracula III Legacy,\n",
       " Kokoda,\n",
       " Halloweentown II: Kalabar's Revenge,\n",
       " South 5,\n",
       " Long Weekend,\n",
       " Prince Valiant,\n",
       " Confessions of a Shopaholic,\n",
       " Bloodshed,\n",
       " Condor, El,\n",
       " South of St. Louis,\n",
       " The Skulls,\n",
       " Stir of Echoes: The Homecoming,\n",
       " The Majestic,\n",
       " Professor Layton and the Eternal Diva,\n",
       " Mama, I Want to Sing!,\n",
       " Padatha Painkili,\n",
       " Night Watch,\n",
       " The Last Day of Summer,\n",
       " The Dukes of Hazzard,\n",
       " The Living End,\n",
       " Santa Baby 2,\n",
       " Merry Christmas, Drake & Josh,\n",
       " The Angels' Share,\n",
       " The Duel at Silver Creek,\n",
       " The Wackness,\n",
       " Suing the Devil,\n",
       " It's a Trap!,\n",
       " The Reef,\n",
       " Robot Chicken: Star Wars Episode II,\n",
       " Killer Movie,\n",
       " Hardwired,\n",
       " The Grind,\n",
       " Blitz,\n",
       " The Movie]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir.query('luke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb33ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[All Quiet on the Western Front,\n",
       " American Drug War: The Last White Hope,\n",
       " O Stratis parastratise,\n",
       " Dobrynya Nikitich and Zmey Gorynych,\n",
       " Kind Hearts and Coronets,\n",
       " Hardware Wars,\n",
       " St. Ives,\n",
       " Fido,\n",
       " Invitation to the Waltz,\n",
       " Alexander the Great,\n",
       " Die Abenteuer des Werner Holt,\n",
       " La Bandera,\n",
       " Paan Singh Tomar,\n",
       " The Emperor's Shadow,\n",
       " Home of the Brave,\n",
       " SpaceCamp,\n",
       " Mask,\n",
       " Life at the End of the Rainbow,\n",
       " Captain Horatio Hornblower,\n",
       " Quo Vadis,\n",
       " From Star Wars to Jedi: The Making of a Saga,\n",
       " G.O.R.A.,\n",
       " Grasshoppers,\n",
       " Mr. and Mrs. Iyer,\n",
       " Zombie Strippers,\n",
       " Cat City,\n",
       " If I Should Fall,\n",
       " St. George Shoots the Dragon,\n",
       " Iluminados Por El Fuego,\n",
       " Time Bandits,\n",
       " Padme,\n",
       " The Saragossa Manuscript,\n",
       " First Knight,\n",
       " The Parade,\n",
       " Gone with the Wind,\n",
       " The Trojan Women,\n",
       " Blue Gold: World Water Wars,\n",
       " Star Wars Episode II: Attack of the Clones,\n",
       " La famiglia,\n",
       " HMS Defiant,\n",
       " Wishology,\n",
       " An Inconvenient Tax,\n",
       " Love and Death,\n",
       " Star Odyssey,\n",
       " Krrish,\n",
       " X-Men Origins: Wolverine,\n",
       " Zack and Miri Make a Porno,\n",
       " The Square Peg,\n",
       " A Time to Love and a Time to Die,\n",
       " Eternity,\n",
       " Piece by Piece,\n",
       " Black Arrow,\n",
       " ReBoot: My Two Bobs,\n",
       " The Congress Dances,\n",
       " Shakti: The Power,\n",
       " The Secret Life of Words,\n",
       " Derelict,\n",
       " Spirit: Stallion of the Cimarron,\n",
       " Anjaneyulu,\n",
       " Secret of the Sultan,\n",
       " Winners,\n",
       " Insaniyat,\n",
       " Los Bandoleros,\n",
       " Why We Fight,\n",
       " Doraemon: Nobita Drifts in the Universe,\n",
       " Persuasion,\n",
       " Real Life,\n",
       " Decision Before Dawn,\n",
       " The English Patient,\n",
       " Underground,\n",
       " Chinatown Kid,\n",
       " The 10th Victim,\n",
       " Charles II: The Power and The Passion,\n",
       " The Fast and the Furious,\n",
       " E,\n",
       " The Star Wars Holiday Special,\n",
       " Zamaana Deewana,\n",
       " Flag Wars,\n",
       " Robot Chicken: Star Wars Episode III,\n",
       " Now and Then,\n",
       " Camp Rock 2: The Final Jam,\n",
       " Bismarck,\n",
       " Soldier Blue,\n",
       " The Young Lions,\n",
       " Hitman Hart: Wrestling with Shadows,\n",
       " The Man in the Iron Mask,\n",
       " The World Moves On,\n",
       " Last Stand of the 300,\n",
       " Stone's War,\n",
       " Neurosia: 50 Years of Perversity,\n",
       " The Countess,\n",
       " Fanboys,\n",
       " 1971,\n",
       " Babylon 5: In the Beginning,\n",
       " DC 9/11: Time of Crisis,\n",
       " The Formula,\n",
       " Nesimi,\n",
       " The Making of Star Wars,\n",
       " El Santo de la Espada,\n",
       " Mob Sister,\n",
       " Seven Beauties,\n",
       " Fahrenheit 9/11,\n",
       " Deal of the Century,\n",
       " Redshirt Blues,\n",
       " Ernest in the Army,\n",
       " The Day the Earth Stood Still,\n",
       " Star Wars Episode III: Revenge of the Sith,\n",
       " Barbarian,\n",
       " Death Sentence,\n",
       " Reign of the Fallen,\n",
       " Welcome to the Space Show,\n",
       " Deathsport,\n",
       " Ramona and Beezus,\n",
       " Classic Creatures: Return of the Jedi,\n",
       " Drug Wars: The Camarena Story,\n",
       " Osama,\n",
       " Young Guns II,\n",
       " Mangal Pandey: The Rising,\n",
       " Ride a Wild Pony,\n",
       " ... nur ein Kom√∂diant,\n",
       " Holy Wars,\n",
       " The Giant of Marathon,\n",
       " Cosmos: War of the Planets,\n",
       " Mean Girls 2,\n",
       " Cromwell,\n",
       " Vidimo se u ƒçitulji,\n",
       " L'affaire Farewell,\n",
       " Argo,\n",
       " Crocodile Dundee in Los Angeles,\n",
       " Noodle,\n",
       " Waisa Bhi Hota Hai Part II,\n",
       " Queen of Outer Space,\n",
       " ATL,\n",
       " Disaster Movie,\n",
       " The Blood of Heroes,\n",
       " Heavy Metal,\n",
       " Scorching Sun, Fierce Winds, Wild Fire,\n",
       " Edipo Alcalde,\n",
       " The Adjustment Bureau,\n",
       " A Time for Killing,\n",
       " Master and Commander: The Far Side of the World,\n",
       " Wyatt Earp,\n",
       " Orlando,\n",
       " Tucker: The Man and His Dream,\n",
       " P'tang, Yang, Kipperbang,\n",
       " Saving Star Wars,\n",
       " Trooper Clerks,\n",
       " The Trial,\n",
       " Ijjodu,\n",
       " Hollywood Sex Wars,\n",
       " King of Hearts,\n",
       " Voyna,\n",
       " The Princess and the Pea,\n",
       " Amazing Grace,\n",
       " Lego Star Wars: The Quest for R2-D2,\n",
       " Boris and Natasha: The Movie,\n",
       " The Last Samurai,\n",
       " For the Cause,\n",
       " Narcosat√°nicos Asesinos,\n",
       " Star Warp'd,\n",
       " D√ºnyayƒ± Kurtaran Adam,\n",
       " D√©sir√©e,\n",
       " Quality Street,\n",
       " The Peacemaker,\n",
       " The Life and Death of Colonel Blimp,\n",
       " The Bubble,\n",
       " H. G. Wells' The Shape of Things to Come,\n",
       " The Weight of Chains,\n",
       " Robot Chicken: Star Wars Episode II,\n",
       " The Battle of Rogue River,\n",
       " Major Payne,\n",
       " Unexpected,\n",
       " The Flower of My Secret,\n",
       " Kolberg,\n",
       " Tumko Na Bhool Payenge,\n",
       " The Wings of Eagles,\n",
       " Dragons: Fire and Ice,\n",
       " Doraemon: Nobita and the New Steel Troops: ~Angel Wings~,\n",
       " LEGO Star Wars: Revenge of the Brick]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir.query('wars')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
